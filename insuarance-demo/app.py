import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

@st.cache_resource
def load_model():
    model = AutoModelForCausalLM.from_pretrained("google/gemma-3-4b-it", device_map="auto", trust_remote_code=True)
    tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-4b-it")
    return pipeline("text-generation", model=model, tokenizer=tokenizer)

st.title("ğŸ“‹ ë³´í—˜ QA ë°ëª¨ (Gemma-4B + LoRA)")
st.write("ë³´í—˜ ë¬¸ì„œì— ëŒ€í•œ ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´, ìì—°ì–´ë¡œ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.")

context = st.text_area("ğŸ“„ ë¬¸ì„œ ìš”ì•½ or ë‚´ìš©", height=200, value="ê³„ì•½ì¼ í¬í•¨ 90ì¼ì´ ì§€ë‚œ ë‚ ì˜ ë‹¤ìŒ ë‚ ë¶€í„° ì•” ë³´ì¥ì´ ì‹œì‘ë©ë‹ˆë‹¤.")
question = st.text_input("â“ ì§ˆë¬¸", "2ë‹¬ ì§€ë‚˜ì„œ ì•” ì§„ë‹¨ ë°›ì•˜ëŠ”ë° ë³´í—˜ê¸ˆ ë°›ì„ ìˆ˜ ìˆì–´?")

if st.button("ì§ˆë¬¸ì— ë‹µí•˜ê¸°"):
    pipe = load_model()
    prompt = f"<|user|>\n{question}\n<context>\n{context}\n<|assistant|>\n"
    output = pipe(prompt, max_new_tokens=200)[0]["generated_text"]
    answer = output.split("<|assistant|>")[-1].strip()
    st.success(f"ğŸ¤– {answer}")
